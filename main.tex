\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{textcomp}
%\usepackage[top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}
% add other packages here

% put your group number and names in the author field
\title{\bf Exercise 5: An Auctioning Agent for the Pickup and Delivery Problem}
\author{Group \textnumero 3: Robin Clerc, Pierre Vigier}

\begin{document}
\maketitle

\section{Planner}

We took some time to optimize our planner because it seems very important us that computing an efficient plan with a low cost is crucial if we want to have a good bidding strategy and be competitive. 

We keep the algorithm of the previous assignment but thanks to the feedback of the assistants we made great improvements on it.

Firstly, we improve the generation of neighbors so that it is faster and its duration is independent of the simulation parameters. This way it is easier to manage the duration of computations and avoid timeouts.

Then, we also modify the stopping condition to stop before the timeout. We failed to do that correctly in the previous assignment.

Finally, we observe that the local optimization was very sensitive to initial conditions. So instead of doing only one long run of stochastic optimization, we do several shorter runs and take the best solutions. This approach turned out to be way more reliable, it almost always gives good results.

\section{Bidding strategy}

For each new task we start by computing the marginal cost with respect to the list of tasks we already have, then we multiply it by a ratio reflecting our strategy.

Our initial ratio is $1+0.01 \frac{timeout_{bid}-timeout_{plan}}{timeout_{bid}}$ Indeed we consider that thanks to more time to compute the final plan we will be able to be even more efficient.

Moreover, we observed that often, once we have a first task, the marginal cost of the next ones is far lower than the first. Our strategy is to be very aggressive for the acquisition of the first task, keep the deficit in memory and compensate it in the next bids. Thanks to it we expect our bid to be still lower than the opponent one as it would be its first task, with a high marginal cost as previously stated.

Furthermore we take into account the last result to update our ratio. If we won the last bid then we are likely to be able to win even if we keep a bigger margin, so we can improve our ratio by 1 percent. On the opposite if we lost the last bid, we should reduce our ratio.

Maybe we should take into account the distribution probability to compute the firsts marginal costs.

\subsection{Pierre - Modeling}

The core idea behind our bidding strategy is that we try to learn the bidding strategy of our adversaries. To do that, we model the bidding strategy of the adversary $i$ by a random variable $B_i$. The probability distribution we choose for $B_i$ is $B_i \sim l\mathcal{N}(\mu_i, \sigma_i)$ where $l$ is the path's length of the task we are bidding for. We choose this model because we think that the ratio of the bid over the path length ($\frac{b}{l}$) reflects the ability to optimize the cost to transport a task.

Then we have to fit the parameters $\mu_i$ and $\sigma_i$ for each adversary. The naive solution is to minimize the negative log-likelihood. However with this method all previous rounds would have the same weight in the process of choosing the values for $\mu_i$ and $\sigma_i$. But as the strategy of the adversaries may evolve and their marginal cost to transport a task may decrease, this strategy does not seem good. Instead we choose to introduce a discount factor $\gamma$, in order to give more weight to the recent rounds. Thus, we use the following loss function:
$$
\mathcal{L}(\mu_i, \sigma_i) = \sum_{j=1}^{N}{\gamma^{N-i}\left(\frac{\frac{b_{i,j}}{l_j} - \mu_i}{2\sigma^2}\right)}
$$
where $b_{i,j}$ is the bid of the agent $i$ during the round $j$ and $N$ the number of rounds so far.

We solve the system below to find the values of $\mu_i$ and $\sigma_i$ that minimize the cost function.
$$
\begin{array}{ccc}
\frac{\partial \mathcal{L}}{\partial \mu_i} = 0 & 
\text{and} &
\frac{\partial \mathcal{L}}{\partial \sigma_i} = 0 \\
\end{array}
$$

And finally, we find:
\begin{equation}
\begin{array}{ccc}
\mu_i = \frac{\sum_{j = 1}^{N}{\gamma^{N-i}\frac{b_{i,j}}{l_j}}}{\sum_{j = 1}^{N}{\gamma^{N-i}}} &
\text{and} &
\sigma_i = \frac{\sum_{j = 1}^{N}{\gamma^{N-i}\left(\frac{b_{i,j}}{l_j}-\mu_i\right)^2}}{\sum_{j = 1}^{N}{\gamma^{N-i}}} \\
\end{array}
\label{update}
\end{equation}

Now, we have access to probability distributions that represent the bidding strategies of the opponents. We will use it to choose our bid. Precisely, we will bid the quantity $b^*_i$ that maximizes our expected reward i.e.:

\begin{equation}
b^*_i = \text{argmax }\mathbb{E}(b\mathds{1}_{B_i > b}) = \text{argmax }bPr(B_i > b)
\label{predict}
\end{equation}

\subsection{Pierre - Bidding strategy}

However the strategy described above is not good for the first rounds for two reasons. The first one is that if we want the values $\mu_i$ and $\sigma_i$ to be meaningful, we should have $N \geq 2$. The second one is that during the first rounds, we may want to be more aggressive to win the first tasks and thus to have a marginal cost that will be probably lower for later tasks.

To to this, we introduce a parameter $N_{warm-up}$, such that when $N < N_{warm-up}$, we use a simpler and more aggressive strategy. And then, when $N \geq N_{warm-up}$ and we have enough information about the adversary bidding strategy, we use the method described above.

Here is our strategy's pseudo code:
\begin{itemize}
\item If $N < N_{warm-up}$, then bid $r \times marginal\_cost$
\item Else
\begin{itemize}
	\item Update $\mu_i$ and $\sigma_i$ for every opponents using the formula \ref{update}
	\item Compute $b^*_i$ for every opponents using the formula \ref{predict}
	\item Compute $b^* = \text{argmin } b^*_i$
	\item If $b^* \geq marginal\_cost$ then bid $b^*$ otherwise bid $marginal\_cost$
\end{itemize}
\end{itemize}

To recap, in our strategy, there is a first phase during which we use a more aggressive strategy. Then after several rounds, we have enough information in order to predict the opponent's bidding and we use this information to try earning the maximum money. However during the second phase, the strategy is more prudent: we do not bid lower than our marginal cost.

% describe in details your bidding strategy. Also, focus on answering the following questions:
% - do you consider the probability distribution of the tasks in defining your strategy? How do you speculate about the future tasks that might be auctions?
% - how do you use the feedback from the previous auctions to derive information about the other competitors?
% - how do you combine all the information from the probability distribution of the tasks, the history and the planner to compute bids?

\section{Results}
% in this section, you describe several results from the experiments with your auctioning agent

\subsection{Experiment 1: Comparisons with dummy agents}
% in this experiment you observe how the results depends on the number of tasks auctioned. You compare with some dummy agents and potentially several versions of your agent (with different internal parameter values).

\subsubsection{Setting}
% you describe how you perform the experiment, the environment and description of the agents you compare with

We use the topology of England and two companies A and B with two vehicles each. 

We write a script to compare the performance of the strategies we designed.
The script pick the vehicles' capacities randomly in $\{20, ..., 100\}$, their cost per kilometer randomly in $\{3, ..., 8\}$ and the initial position of the vehicles randomly. Then the script launches the simulation, get the results, exchange the companies' vehicles and relaunch the simulation for fairness. It does so several times to get more reliable results.

\subsubsection{Observations}
% you describe the experimental results and the conclusions you inferred from these results

In the table below, there are the results of the strategy described in the previous parts against dummies strategies that we have designed. The number we report in each cell is the total reward of our strategy minus the total reward of the dummy strategy over several runs.

\begin{center}
\begin{tabular}{|c|c|}
\hline
Strategy & \textit{best strategy} \\
\hline
\textit{fixed ratio} & 12725 \\
\hline
\textit{adaptive ratio 1} & 27658 \\
\hline
\textit{adaptive ratio 2} & 30191 \\
\hline
\end{tabular}
\end{center}

\subsection{Experiment n}
% other experiments you would like to present (for example, varying the internal parameter values)

\subsubsection{Setting}

\subsubsection{Observations}

\end{document}
